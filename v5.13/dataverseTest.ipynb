{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "The purpose of this document is to create a Dataverse API testing notebook, in addition to providing curation workflow for data deposit into a repository (only the deposit and prep portion of the workflow). The tests written for this notebook are being run against Dataverse v5.13.\n",
    "\n",
    "See [_about_dataverseTest.md](./_about_dataverseTest.md) for information about configuring and running this notebook, and the technical details about the notebook (since we didn't want to bog down the notebook with instructions if you know Python).\n",
    "\n",
    "See the `CHANGELOG.md` file for issues needing to be addressed and recent changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** RESTART THE NOTEBOOK KERNEL IF YOU MAKE EDITS TO THE _worker_modTest.py script or configuration ***\n",
    "\n",
    "# run the _installer_dataverseTest.py script and import our _worker_modTest.py script\n",
    "import _installer_dataverseTest\n",
    "# %load_ext autoreload  # do not use this with the 'logger' plugin otherwise duplicate logging messages will appear\n",
    "# %autoreload all\n",
    "from _worker_dataverseTest import Worker\n",
    "# we need the 'autoreload' above if we are actively making changes to the worker.py module and want to reload any changes to the module without restarting the notebook kernel\n",
    "# NOTE: if we make changes to the worker script or configuration we need to rerun this code block for the notebook to use the new edits\n",
    "objWorker = Worker(\"_config_dataverseTest.json\") # initialize our Worker object; we should only need to call this once for the notebook session (working with 'demo' configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the notebook code\n",
    "\n",
    "The code blocks in this notebook are intentionally brief because most users are not concerned with what the code looks like (at least initially). If you want to know what the scripts do then review the .py files that we imported into this notebook. However we will briefly describe a line of code so you have a general idea of what is happening behind the scenes.\n",
    "\n",
    "The `objWorker.ObjDvApi.DvCreateCollection()` command for example, runs the `DvCreateCollection()` method, which is found in the `ObjDvApi` object, and makes a Dataverse API request to create a new repository/collection. The `ObjDvApi` is simply defined in an external Python file which contains reusable methods for working with the Dataverse API. We use this same class for all of our datasets, so keeping the methods in a single file for reuse is better than manually adding into the code of each of our datasets and making our working code script more densely worded than it needs to be.\n",
    "\n",
    "### The objWorker\n",
    "\n",
    "The `objWorker` is the object that we customize for each dataset and simply acts as a template for importing different classes/objects we want to attach to it. For instance, we attach the `ObjDvApi` to our `objWorker` object so whatever functionality exists in the `ObjDvApi` class can be used in our `objWorker` class. The `.` between `objWorker.ObjDvApi` simply represents that `ObjDvApi` is an extension of `objWorker`. An analogy would be adding a dustpan to a broom (or `broom.dustpan`) to extend the functionality of the broom, so the broom can now be used to pick up dust and not simply push it around.\n",
    "\n",
    "Below are some simple code commands to set up a Dataverse collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataverse Collection\n",
    "\n",
    "### Configuration\n",
    "\n",
    "The `_config_dataverseTest.json` file contains a Dataverse starter object `objDvApi_COLLECTION_START` to create a new Dataverse collection. Luckily we do not need to follow the API documentation that instructs users to create a separate JSON file for use with the API endpoint. Since we added the JSON to our main configuration file we can simply reference the object in the `json` parameter of our request. We will place this collection under the root 'parent' collection.\n",
    "\n",
    "### Retrieving our collection info\n",
    "\n",
    "Since we already have our starter collection information defined in our main `_config_dataverseTest.json` file, there is no need to save the collection information sent back from the creation of our collection. We can always use the `ViewCollection()` method in our worker script to retrieve the collection information as long as we at least know our collection alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objWorker.createCollection()  # initialize a new collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objWorker.viewCollection()  # view information on our dataverse collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objWorker.deleteCollection()  # delete our dataverse collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objWorker.viewCollectionContents()  # view dataverse collection contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset\n",
    "\n",
    "Using the https://guides.dataverse.org/en/5.13/_downloads/4e04c8120d51efab20e480c6427f139c/dataset-create-new-all-default-fields.json referenced in https://guides.dataverse.org/en/5.13/api/native-api.html#create-a-dataset-in-a-dataverse-collection, will be our dataset template. We simply add this JSON object to our `_config_dataverseTest.json` file under the `DATAVERSE_DATASET` constant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objWorker.createDataset()  # create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objWorker.deleteDataset()  # delete dataset draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create fake data\n",
    "\n",
    "Next we need to create some files to test the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objWorker.createTestFiles(\"lstTEST_FILES\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding files to the dataset\n",
    "\n",
    "The Dataverse API guide is confusing when it comes to handling files, but we have designed the `ObjDvApi` class to handle this for you. However if you want to know how it works read on.\n",
    "\n",
    "### Adding a file that does not exist in the dataset\n",
    "\n",
    "If adding a new file (based on file name), that does not currently exist in the dataset, then use the `add file` API endpoint. \n",
    "\n",
    "### Replacing a file that exists in the dataset\n",
    "\n",
    "#### A file with the same content exists in the dataset (regardless of metadata)\n",
    "\n",
    "The Dataverse will not allow you to upload a file that currently exists in the dataset with the same MD5 checksum (same content), however you can replace the metadata for the file. To do this you must use the .\n",
    "\n",
    "#### File with differing content (regardless of metadata)\n",
    "\n",
    "If uploading a file that already exists in the dataset you should use the `file replace` API endpoint otherwise using the `add file` endpoint will create a duplicate file in your dataset (which you do not want).\n",
    "\n",
    "When we upload a file to a dataset, it is advisable to check the MD5 hash of the file you are attempting to upload. Our `ObjDvApi` class handles this for you. If the MD5 hash is the same and you upload the file to the dataset, then a new file will be added to the dataset with a file name ending in a number. Thus you will end up with two duplicate files in the dataset with two different names (which you should not do). We have added an MD5 hash checking method to our `ObjDvApi` class that will check for matching MD5 hashes and will use the `file replace` API if files already exist in the dataset.\n",
    "\n",
    "**Note: uploading new files (different MD5 hashes) to a dataset draft with existing files of the same names will result in duplicate files being added, so we need to use the `file replace` API instead for existing files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objWorker.uploadTestFiles(\"lstTEST_FILES\") # initial list of files to upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish dataset\n",
    "\n",
    "https://guides.dataverse.org/en/5.13/api/native-api.html#publish-a-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objWorker.publishDatasetDraft(\"major\") # we need to determine if the dataverse is published before trying to publish it again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset version test\n",
    "\n",
    "Next we will create another set of test files and use them to update the dataset version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objWorker.createTestFiles(\"lstTEST_FILES2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we upload the replacement files we need to delete the files we do not want https://guides.dataverse.org/en/latest/api/native-api.html#deleting-files\n",
    "# or delete after we upload\n",
    "\n",
    "objWorker.uploadTestFiles(\"lstTEST_FILES2\") # see if we can create a new version of the dataset with a different set of files\n",
    "objWorker.removeUnusedFiles(\"lstTEST_FILES2\") # remove any old dataset files we no longer need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List files in a dataset\n",
    "\n",
    "Once we have added our new files to the dataset we want to see a list of all files in the draft (make sure to use one of the version specifiers listed in https://guides.dataverse.org/en/latest/api/native-api.html#dataset-version-specifiers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objWorker.viewDatasetFiles(\":draft\")  # show dataset contents of our draft since this is the version we are interested in for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
