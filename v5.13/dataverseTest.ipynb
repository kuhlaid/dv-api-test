{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "The purpose of this document is to create a Dataverse API testing notebook. These tests are being run against Dataverse v5.13.\n",
    "\n",
    "See [_about_dataverseTest.md](./_about_dataverseTest.md) for information about configuring and running this notebook, and the technical details about the notebook (since we didn't want to bog down the notebook with instructions if you know Python).\n",
    "\n",
    "See the `CHANGELOG.md` file for issues needing to be addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataverse Collection\n",
    "\n",
    "### Configuration\n",
    "\n",
    "Using the Dataverse starter object `DATAVERSE_COLLECTION_START` in our configuration file we will create a new collection through the API https://guides.dataverse.org/en/5.13/api/native-api.html#create-a-dataverse-collection. Luckily we do not need to follow the API documentation that instructs users to create a separate JSON file for use with the API endpoint. Since we added the JSON to our main configuration file we can simply reference the object in the `json` parameter of our request. We will place this collection under the root 'parent' collection.\n",
    "\n",
    "### Retrieving our collection info\n",
    "\n",
    "Since we already have our starter collection information defined in our main `_config_dataverseTest.json` file, there is no need to save the collection information sent back from the creation of our collection. We can always use the `ViewCollection()` method in our worker script to retrieve the collection information as long as we at least know our collection alias. \n",
    "\n",
    "### Issue\n",
    "\n",
    "Note: If you use a GET request instead of a POST request to the API endpoint, the action may appear to be successful but it will simply be returning the Dataverse collection of the main parent collection, and NOT create a new collection for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Faker in /opt/conda/lib/python3.11/site-packages (28.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /opt/conda/lib/python3.11/site-packages (from Faker) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.4->Faker) (1.16.0)\n",
      "Requirement already satisfied: curlify in /opt/conda/lib/python3.11/site-packages (2.2.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from curlify) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->curlify) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->curlify) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->curlify) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->curlify) (2024.2.2)\n",
      "Collecting DvApiMod_pip_package@ git+https://github.com/kuhlaid/DvApiMod5.13.git\n",
      "  Cloning https://github.com/kuhlaid/DvApiMod5.13.git to /tmp/pip-install-d32hu4ca/dvapimod-pip-package_a56923c3d5e9489b812a70e0edac6176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/kuhlaid/DvApiMod5.13.git /tmp/pip-install-d32hu4ca/dvapimod-pip-package_a56923c3d5e9489b812a70e0edac6176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Resolved https://github.com/kuhlaid/DvApiMod5.13.git to commit fdb481afb1262b9de98221b314cf29365eb44554\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: DvApiMod_pip_package\n",
      "  Building wheel for DvApiMod_pip_package (setup.py): started\n",
      "  Building wheel for DvApiMod_pip_package (setup.py): finished with status 'done'\n",
      "  Created wheel for DvApiMod_pip_package: filename=DvApiMod_pip_package-1.0-py3-none-any.whl size=1993 sha256=4b2a84a8668d7e55086ef018dc68456d90823abd626a8ab8878fee9fd3b1f94e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-tv_4ubu2/wheels/2d/2b/6a/e26e442182023f1df598097e62088495e948911c707b25907b\n",
      "Successfully built DvApiMod_pip_package\n",
      "Installing collected packages: DvApiMod_pip_package\n",
      "Successfully installed DvApiMod_pip_package-1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-27 20:34:46 EST DvApiMod [INFO] Finished ObjDvApi init\n",
      "2024-08-27 20:34:46 EST _worker_dataverseTest [INFO] Finished installing and importing modules for the _config_dataverseTest.json environment\n"
     ]
    }
   ],
   "source": [
    "# *** RESTART THE NOTEBOOK KERNEL IF YOU MAKE EDITS TO THE _worker_modTest.py script or configuration ***\n",
    "\n",
    "# run the _installer_dataverseTest.py script and import our _worker_modTest.py script\n",
    "import _installer_dataverseTest\n",
    "# %load_ext autoreload  # do not use this with the 'logger' plugin otherwise duplicate logging messages will appear\n",
    "# %autoreload all\n",
    "from _worker_dataverseTest import Worker\n",
    "# we need the 'autoreload' above if we are actively making changes to the worker.py module and want to reload any changes to the module without restarting the notebook kernel\n",
    "# NOTE: if we make changes to the worker script or configuration we need to rerun this code block for the notebook to use the new edits\n",
    "objWorker = Worker(\"_config_dataverseTest.json\") # initialize our Worker object; we should only need to call this once for the notebook session (working with 'demo' configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the notebook code\n",
    "\n",
    "The code blocks in this notebook are intentionally brief because most users are not concerned with what the code looks like (at least initially). If you want to know what the scripts do then review the .py files that we imported into this notebook. However we will briefly describe a line of code so you have a general idea of what is happening behind the scenes.\n",
    "\n",
    "The `objWorker.ObjDvApi.DvCreateCollection()` command for example, runs the `DvCreateCollection()` method, which is found in the `ObjDvApi` object, and makes a Dataverse API request to create a new repository/collection. The `ObjDvApi` is simply defined in an external Python file which contains reusable methods for working with the Dataverse API. We use this same class for all of our datasets, so keeping the methods in a single file for reuse is better than manually adding into the code of each of our datasets and making our working code script more densely worded than it needs to be.\n",
    "\n",
    "### The objWorker\n",
    "\n",
    "The `objWorker` is the object that we customize for each dataset and simply acts as a template for importing different classes/objects we want to attach to it. For instance, we attach the `ObjDvApi` to our `objWorker` object so whatever functionality exists in the `ObjDvApi` class can be used in our `objWorker` class. The `.` between `objWorker.ObjDvApi` simply represents that `ObjDvApi` is an extension of `objWorker`. An analogy would be adding a dustpan to a broom (or `broom.dustpan`) to extend the functionality of the broom, so the broom can now be used to pick up dust and not simply push it around.\n",
    "\n",
    "Below are some simple code commands to set up a Dataverse collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objWorker.ObjDvApi.createCollection()  # initialize a new collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-27 20:34:46 EST DvApiMod [INFO] start viewCollection\n",
      "2024-08-27 20:34:46 EST DvApiMod [INFO] making request: https://demo-dataverse.rdmc.unc.edu/api/dataverses/jocoknow\n",
      "2024-08-27 20:34:46 EST DvApiMod [INFO] ----------------------------------------\n",
      "2024-08-27 20:34:46 EST DvApiMod [INFO] response status=200\n",
      "2024-08-27 20:34:46 EST DvApiMod [INFO] headers={'Date': 'Wed, 28 Aug 2024 00:36:20 GMT', 'Server': 'Apache/2.4.37 (Rocky Linux) OpenSSL/1.1.1k', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Methods': 'PUT, GET, POST, DELETE, OPTIONS', 'Access-Control-Allow-Headers': 'Accept, Content-Type, X-Dataverse-Key, Range', 'Access-Control-Expose-Headers': 'Accept-Ranges, Content-Range, Content-Encoding', 'Content-Type': 'application/json;charset=UTF-8', 'Content-Length': '424', 'Keep-Alive': 'timeout=5, max=100', 'Connection': 'Keep-Alive'}\n",
      "2024-08-27 20:34:46 EST DvApiMod [INFO] json={'status': 'OK', 'data': {'id': 391, 'alias': 'jocoknow', 'name': 'JoCoKnow', 'affiliation': 'University of North Carolina - Chapel Hill', 'dataverseContacts': [{'displayOrder': 0, 'contactEmail': 'w.patrick.gale@unc.edu'}, {'displayOrder': 1, 'contactEmail': 'student@example.edu'}], 'permissionRoot': True, 'description': 'JoCoOA and JoCoHS data repository', 'dataverseType': 'RESEARCH_PROJECTS', 'ownerId': 1, 'creationDate': '2024-08-19T19:23:03Z'}}\n",
      "2024-08-27 20:34:46 EST DvApiMod [INFO] end viewCollection\n"
     ]
    }
   ],
   "source": [
    "objWorker.ObjDvApi.viewCollection()  # view information on our dataverse collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objWorker.ObjDvApi.deleteCollection()  # delete our dataverse collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objWorker.ObjDvApi.viewCollectionContents()  # view dataverse collection contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset\n",
    "\n",
    "Using the https://guides.dataverse.org/en/5.13/_downloads/4e04c8120d51efab20e480c6427f139c/dataset-create-new-all-default-fields.json referenced in https://guides.dataverse.org/en/5.13/api/native-api.html#create-a-dataset-in-a-dataverse-collection, will be our dataset template. We simply add this JSON object to our `_config_dataverseTest.json` file under the `DATAVERSE_DATASET` constant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objWorker.createDataset()  # create a partial dataset\n",
    "# NOTE: we need the JSON object of the full dataset initialization (waiting for RDMC response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objWorker.deleteDataset()  # delete dataset draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create fake data\n",
    "\n",
    "Next we need to create some files to test the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objWorker.createTestFiles(\"lstTEST_FILES\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding files to the dataset\n",
    "\n",
    "The Dataverse API guide is confusing when it comes to handling files, but we have designed the `ObjDvApi` class to handle this for you. However if you want to know how it works read on.\n",
    "\n",
    "### Adding a file that does not exist in the dataset\n",
    "\n",
    "If adding a new file (based on file name), that does not currently exist in the dataset, then use the `add file` API endpoint. \n",
    "\n",
    "### Replacing a file that exists in the dataset\n",
    "\n",
    "#### A file with the same content exists in the dataset (regardless of metadata)\n",
    "\n",
    "The Dataverse will not allow you to upload a file that currently exists in the dataset with the same MD5 checksum (same content), however you can replace the metadata for the file. To do this you must use the .\n",
    "\n",
    "#### File with differing content (regardless of metadata)\n",
    "\n",
    "If uploading a file that already exists in the dataset you should use the `file replace` API endpoint otherwise using the `add file` endpoint will create a duplicate file in your dataset (which you do not want).\n",
    "\n",
    "When we upload a file to a dataset, it is advisable to check the MD5 hash of the file you are attempting to upload. Our `ObjDvApi` class handles this for you. If the MD5 hash is the same and you upload the file to the dataset, then a new file will be added to the dataset with a file name ending in a number. Thus you will end up with two duplicate files in the dataset with two different names (which you should not do). We have added an MD5 hash checking method to our `ObjDvApi` class that will check for matching MD5 hashes and will use the `file replace` API if files already exist in the dataset.\n",
    "\n",
    "**Note: uploading new files (different MD5 hashes) to a dataset draft with existing files of the same names will result in duplicate files being added, so we need to use the `file replace` API instead for existing files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objWorker.uploadTestFiles(\"lstTEST_FILES\") # initial list of files to upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish dataset\n",
    "\n",
    "https://guides.dataverse.org/en/5.13/api/native-api.html#publish-a-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objWorker.publishDatasetDraft(\"major\") # we need to determine if the dataverse is published before trying to publish it again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset version test\n",
    "\n",
    "Next we will create another set of test files and use them to update the dataset version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objWorker.createTestFiles(\"lstTEST_FILES2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we upload the replacement files we need to delete the files we do not want https://guides.dataverse.org/en/latest/api/native-api.html#deleting-files\n",
    "# or delete after we upload\n",
    "\n",
    "objWorker.uploadTestFiles(\"lstTEST_FILES2\") # see if we can create a new version of the dataset with a different set of files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List files in a dataset\n",
    "\n",
    "Once we have added our new files to the dataset we want to see a list of all files in the draft (make sure to use one of the version specifiers listed in https://guides.dataverse.org/en/latest/api/native-api.html#dataset-version-specifiers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objWorker.viewDatasetFiles(\":draft\")  # show dataset contents of our draft since this is the version we are interested in for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting files\n",
    "\n",
    "This is supported in later versions of the Dataverse but not v5.13.\n",
    "https://guides.dataverse.org/en/latest/api/native-api.html#deleting-files\n",
    "\n",
    "One alternative that might work for v5.13 is to replace the files (https://guides.dataverse.org/en/5.13/api/native-api.html#replacing-files) with a dummy text file and give it an extension of `.empty` and no content.\n",
    "\n",
    "Now we want to make sure can remove any old dataset files we no longer need. To do this we first need to upload the new set of files we want. Next we delete any files that are not in the latest list of files we want saved to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objWorker.removeUnusedFiles(\"lstTEST_FILES2\",\":draft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issue UNCDVSUP-38 (submitted on 8/17)\n",
    "\n",
    "I’m trying to use the JSON from https://guides.dataverse.org/en/5.13/_downloads/4e04c8120d51efab20e480c6427f139c/dataset-create-new-all-default-fields.json to create a new dataset in http://demo-dataverse.rdmc.unc.edu .  However I am receiving an error that makes it seem that the JSON properties are incorrectly defined. Below is the response information (with the error message appearing in https://github.com/IQSS/dataverse.harvard.edu/issues/172 ):\n",
    "\n",
    "json= {'status': 'ERROR', 'message': 'Error parsing Json: incorrect multiple   for field productionPlace'}\n",
    "headers= {'Date': 'Sat, 17 Aug 2024 16:09:07 GMT', 'Server': 'Apache/2.4.37 (Rocky Linux) OpenSSL/1.1.1k', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Methods': 'PUT, GET, POST, DELETE, OPTIONS', 'Access-Control-Allow-Headers': 'Accept, Content-Type, X-Dataverse-Key, Range', 'Access-Control-Expose-Headers': 'Accept-Ranges, Content-Range, Content-Encoding', 'Content-Type': 'application/json;charset=UTF-8', 'Content-Length': '97', 'Connection': 'close'}\n",
    "response status= 400\n",
    "\n",
    "The JSON in question seems to be:\n",
    "\n",
    "{\n",
    "              \"typeName\": \"productionPlace\",\n",
    "              \"multiple\": false,\n",
    "              \"typeClass\": \"primitive\",\n",
    "              \"value\": \"ProductionPlace\"\n",
    "            },\n",
    "\n",
    "The release notes for 5.13 state: \n",
    "\n",
    "Edit the following line to your schema.xml (to indicate that productionPlace is now multiValued='true\"):\n",
    "\n",
    "So I can’t tell if the UNC Dataverse schema simply needs updating or something else is going on. If I set \"multiple\": true, in the JSON then the response is:\n",
    "\n",
    "json= {'status': 'ERROR', 'message': 'Error parsing Json: Invalid values submitted for productionPlace. It should be an array of values.'}\n",
    "\n",
    "…but I do not know how to format the JSON for multiple values.\n",
    "\n",
    "My Python method for creating the dataset is using POST so that should not be the issue.\n",
    "\n",
    "def DvCreateDataset(self):\n",
    "        print(\"start DvCreateDataset\")\n",
    "        strApiEndpoint = '%s/api/dataverses/%s/datasets' % (self.strDATAVERSE_DOMAIN, self._config[\"DATAVERSE_COLLECTION_START\"][\"alias\"])\n",
    "        print('making request: %s' % strApiEndpoint)\n",
    "        objHeaders = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"X-Dataverse-Key\": self.strDATAVERSE_API_TOKEN\n",
    "        }\n",
    "        r = requests.request(\"POST\", strApiEndpoint, json=self._config[\"DATAVERSE_DATASET\"], headers=objHeaders)\n",
    "        self.printResponseInfo(r)\n",
    "        print(\"end DvCreateDataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
